{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The NLTK module is a huge toolkit designed to help you with the entire Natural Language Processing (NLP) approach. NLTK will provide you with everything from splitting paragraphs to sentences, splitting words, identifying the part of speech, highlighting themes, and even helping your machine understand what the text is about. In this series, we will address the areas of opinion mining or sentiment analysis.\n",
        "\n",
        "As we learn how to use NLTK for sentiment analysis, we will learn the following:\n",
        "\n",
        "Participle — Splits the text body into sentences and words.\n",
        "Part of speech tagging\n",
        "Machine learning and naive Bayes classifier\n",
        "How to use Scikit Learn (sklearn) with NLTK together\n",
        "Training classifiers with data sets\n",
        "Real-time streaming sentiment analysis with Twitter.\n",
        "…and more."
      ],
      "metadata": {
        "id": "iojnkwFzJ9hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCmDRhFqKSWb",
        "outputId": "4887eae5-a15b-47cd-e446-636b9201ab10"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to install some components for NLTK. Open python in any of your usual ways and type:\n"
      ],
      "metadata": {
        "id": "fLmWjNfxKVKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select Download All for all packages and click Download. This will give you all the word breakers, blockers, other algorithms and all the corpora. If space is an issue, you can choose to manually download all content. The NLTK module will take up approximately 7MB and the entire nltk_data directory will occupy approximately nltk_data , including your nltk_data , parser and corpus.\n",
        "\n",
        "If you are running a headless version with VPS, you can install everything by running Python and doing the following:\n"
      ],
      "metadata": {
        "id": "A26r7IZJKl8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download()\n",
        "d (for download)\n",
        "All (for download everything)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "4INiJyeNKoHH",
        "outputId": "bad06f96-6c8a-402a-9a7c-49dfacc32cc5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-6-08e453590c92>, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-08e453590c92>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    d (for download)\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will download everything for you.\n",
        "\n",
        "Now that you have all the things you need, let’s type some simple words:\n",
        "\n",
        "Corpus — The body of the text, singular. Corpora is its plural. Example: A collection of medical journals .\n",
        "Lexicon — vocabulary and its meaning. For example: English dictionary. However, considering that there are different thesaurus in each field. For example, for financial investors, the first meaning of the word Bull is the person who is confident in the market. Compared with the “common English vocabulary”, the first meaning of the word is animal. Therefore, financial investors, doctors, children, mechanics, etc. all have a special vocabulary.\n",
        "Token — Each “entity” is part of a rule based split. For example, when a sentence is “split” into words, each word is a tag. If you split a paragraph into sentences, each sentence can also be a marker.\n",
        "These are the most frequently heard words when entering the natural language processing (NLP) field, but we will cover more words in time. So, let’s show an example of how to split something into tags with the NLTK module.\n"
      ],
      "metadata": {
        "id": "6epetDtLLAOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-x0DTNoGLdev",
        "outputId": "94abf86b-bafc-45a6-f518-b6a3bc732d87"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "EXAMPLE_TEXT = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\"\n",
        "print(sent_tokenize(EXAMPLE_TEXT))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ww6_QQfBLUo6",
        "outputId": "018146dc-c9bb-4f18-87bc-2ef5e017eb8f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello Mr. Smith, how are you doing today?', 'The weather is great, and Python is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At first, you might think that word segmentation by word or sentence is quite a trivial matter. For many sentences, it may be. The first step might be to execute a simple .split('. ') , or by a period, followed by a space split. Then maybe you will introduce some regular expressions, divided by periods, spaces, and then uppercase letters. The problem is something like Mr. Smith , and there are many other things that will cause you trouble. Segmentation by word is also a challenge, especially when considering abbreviations, such as we and we're . NLTK saves you a lot of time with this seemingly simple but very complicated operation.\n",
        "\n",
        "The above code will output the sentence, divided into a list of sentences, you can use the for loop to traverse.\n"
      ],
      "metadata": {
        "id": "i4nPHJnTLmxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So here we create the tags, they are all sentences. Let us divide the words by word this time."
      ],
      "metadata": {
        "id": "i8XP5khkLqi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (word_tokenize(EXAMPLE_TEXT))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tLj8-zxLnev",
        "outputId": "e47be544-54c8-4b0a-e072-fa0b94695624"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a few things to note here. First, notice that punctuation is treated as a separate tag. Also, note that the word shouldn't separated into should and n't . The last thing to note is that pinkish-blue is indeed treated as \"a word\", which is what it is. Cool!\n",
        "\n",
        "Now, looking at the words after these participles, we must start thinking about what our next step might be. We began to think about how to get the meaning by observing these words. We can think about how to put value on many words, but we also see some words that are basically worthless. This is a form of “stop word” that we can also handle. This is what we will discuss in the next tutorial."
      ],
      "metadata": {
        "id": "zNN-Tri7L0l6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "II. NLTK and stop words\n",
        "The idea of ​​natural language processing is to perform some form of analysis or processing. The machine can at least understand the meaning, representation or suggestion of the text to some extent.\n",
        "\n",
        "This is obviously a huge challenge, but there are steps that anyone can follow. However, the main idea is that computers don’t understand words directly. It is shocking that humans will not. In humans, memory is broken down into electrical signals in the brain in the form of a neural group that emits patterns. There are still many unknown things about the brain, but the more we break down the human brain into basic elements, we will find the basic elements. Well, it turns out that computers store information in a very similar way! If we want to mimic how humans read and understand text, we need a way that is as close as possible. In general, computers use numbers to represent everything, but we often see the use of binary signals directly in programming ( True or False , which can be directly converted to 1 or 0, directly from the presence of an electrical signal (True, 1) or not Exist (False, 0) ). To do this, we need a way to convert words to numeric or signal patterns. Converting data into something that a computer can understand is called \"preprocessing.\" One of the main forms of preprocessing is to filter out useless data. In natural language processing, useless words (data) are called stop words.\n",
        "\n",
        "We can immediately realize that some words are more meaningful than others. We can also see that some words are useless and are filled words. For example, we use them in English to fill sentences, so there is no such strange sound. One of the most common, unofficial, useless examples is the word umm . People often use umm to fill, more than other words. This word is meaningless unless we are looking for someone who may lack confidence, confusion, or not much. We all do this, there are... oh... many times, you can hear me say umm or uhh in the video. For most analyses, these words are useless.\n",
        "\n",
        "We don’t want these words to take up space in our database or take up valuable processing time. Therefore, we call these words “useless words” because they are useless and we want to treat them. Another version of the word “stop word” can be written more: the words we stop at.\n",
        "\n",
        "For example, if you find words that are often used for satire, you may want to stop immediately. Satirical words or phrases will vary depending on the thesaurus and corpus. For the time being, we will treat stop words as words that do not contain any meaning, and we will remove them.\n",
        "\n",
        "You can do it easily by storing a list of words that you think are stop words. NLTK uses a bunch of words that they think are stop words to get you started, you can access it through the NLTK corpus:\n",
        "\n"
      ],
      "metadata": {
        "id": "PsDo6yQOMa-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6zV8vhIMn5I",
        "outputId": "7893175e-f4f6-4b4b-eedf-26dd6b33ff30"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "5-Q4SioFMbpo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set (stopwords. words ( 'english' ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXqVf8rEL1R1",
        "outputId": "f483fd38-0052-4151-d229-1f4277e50902"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s how to use the stop_words collection to remove stop words from the text:"
      ],
      "metadata": {
        "id": "NfxVs-kAMwFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
        "stop_words = set(stopwords.words( 'english' ))\n",
        "word_tokens = word_tokenize(example_sent)\n",
        "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "filtered_sentence = []\n",
        "for w in word_tokens:\n",
        "   if w not in stop_words:\n",
        "      filtered_sentence.append(w)\n",
        "print(word_tokens)\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASSKdjZ4Mw0M",
        "outputId": "3b25e95d-4893-453c-a796-e736c34604e3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
            "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "III. NLTK stem extraction\n",
        "The concept of stemming is a standardized approach. In addition to the tense, many variations of the word have the same meaning.\n",
        "\n",
        "The reason we extract the stem is to shorten the time of the search and normalize the sentence.\n",
        "\n",
        "consider:\n",
        "\n",
        "\n",
        "I was taking a ride in the car.\n",
        "I was riding in the car.\n",
        "These two sentences mean the same thing. in the car (in the car) is the same. I (I) is the same. In both cases, ing clearly expresses the past tense, so in the case of trying to figure out the meaning of this past-style activity, is it really necessary to distinguish between riding and taking a ride ?\n",
        "\n",
        "No, not.\n",
        "\n",
        "This is just a small example, but imagine every word in English that can be placed on every possible tense and affix on the word. Each version has a separate dictionary entry that will be very redundant and inefficient, especially since once we convert to numbers, the “value” will be the same.\n",
        "\n",
        "One of the most popular porcelain extraction algorithms is Porter, which existed in 1979.\n",
        "\n",
        "First, we have to crawl and define our stems:"
      ],
      "metadata": {
        "id": "JtoDBI8bOZOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "WmEp_20QOeG3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s choose some words with similar stems, for example:"
      ],
      "metadata": {
        "id": "l8OR0ImxOk0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_words = [ \"python\" , \"pythoner\" , \"pythoning\" , \"pythoned\" , \"pythonly\" ]"
      ],
      "metadata": {
        "id": "pnwWJwV4OmyC"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for w in example_words:\n",
        "   print (ps.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BgeAdhnOrOk",
        "outputId": "1bb1c8ac-7ae5-4e12-eb7d-8138a6dbacde"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python\n",
            "python\n",
            "python\n",
            "python\n",
            "pythonli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s try to extract stems from a typical sentence instead of some words:"
      ],
      "metadata": {
        "id": "pcQHMcRLOx21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = \"It is important to by very pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\"\n",
        "words = word_tokenize(new_text)\n",
        "for w in words :\n",
        "   print(ps.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taFHHBd7O09i",
        "outputId": "895b6544-48fd-4a72-b41d-e11e9d620c42"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it\n",
            "is\n",
            "import\n",
            "to\n",
            "by\n",
            "veri\n",
            "pythonli\n",
            "while\n",
            "you\n",
            "are\n",
            "python\n",
            "with\n",
            "python\n",
            ".\n",
            "all\n",
            "python\n",
            "have\n",
            "python\n",
            "poorli\n",
            "at\n",
            "least\n",
            "onc\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we’ll discuss some of the more advanced content of the NLTK module, part-of-speech tagging, where we can use the NLTK module to identify the part of speech of each word in the sentence.\n",
        "\n",
        "IV. NLTK part-of-speech tagging\n",
        "A more powerful aspect of the NLTK module is that it can be used for your part-of-speech tagging. It means to mark words in a sentence as nouns, adjectives, verbs, etc. Even more impressive is that it can also be marked by tense, and others. This is a list of tags, their meaning and some examples:\n",
        "\n",
        "POS tag list:\n",
        " CC coordinating conjunction\n",
        " CD cardinal digit\n",
        " DT determiner\n",
        " EX existential there ( like : \"there is\" ... think of it like \"there exists\" )\n",
        " FW foreign word\n",
        " IN preposition/subordinating conjunction\n",
        " JJ adjective 'big'\n",
        " JJR adjective, comparative 'bigger'\n",
        " JJS adjective, superlative 'biggest'\n",
        " LS list marker 1 )\n",
        " MD modal could, will\n",
        " NN noun, singular 'desk'\n",
        " NNS noun plural 'desks'\n",
        " NNP proper noun, singular 'Harrison'\n",
        " NNPS proper noun, plural 'Americans'\n",
        " PDT predeterminer 'all the kids'\n",
        " POS possessive ending parent 's\n",
        " PRP personal pronoun I, he, she\n",
        " PRP$ possessive pronoun my, his, hers\n",
        " RB adverb very, silently,\n",
        " RBR adverb, comparative better\n",
        " RBS adverb, superlative best\n",
        " RP particle give up\n",
        " TO to go 'to' the store.\n",
        " UH interjection errrrrrrrm\n",
        " VB verb, base form take\n",
        " VBD verb, past tense took\n",
        " VBG verb, gerund/present participle taking\n",
        " VBN verb, past participle taken\n",
        " VBP verb, sing. present, non- 3 d take\n",
        " VBZ verb, 3 rd person sing. present takes\n",
        " WDT wh-determiner which\n",
        " WP wh-pronoun who, what\n",
        " WP$ possessive wh-pronoun whose\n",
        " WRB wh-abverb where , when\n",
        "How do we use this? When we deal with it, we have to explain a new sentence marker called PunktSentenceTokenizer . This marker enables unsupervised machine learning, so you can actually train on any text you use. First, let's get some imports that we plan to use:"
      ],
      "metadata": {
        "id": "QAzTTC5DO-DF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer"
      ],
      "metadata": {
        "id": "uoqehFlFO9wF"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s create the training and test data:"
      ],
      "metadata": {
        "id": "37gOTBkxPH2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('state_union')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAMIUADZPOKa",
        "outputId": "bcceed18-0315-4304-8842-9b2a397708d5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/state_union.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_text = state_union.raw( \"2005-GWBush.txt\" )\n",
        "sample_text = state_union.raw( \"2006-GWBush.txt\" )"
      ],
      "metadata": {
        "id": "AY6X4QtoPIMY"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One is the State of the Union speech since 2005, and the other is the speech of President George W. Bush since 2006.\n",
        "\n",
        "Next, we can train the Punkt marker as follows:\n"
      ],
      "metadata": {
        "id": "UmHdXKEDPSAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)"
      ],
      "metadata": {
        "id": "RiIWDD7xPUmW"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
      ],
      "metadata": {
        "id": "fhnCFl9PPYqY"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can complete the part-of-speech tagging script by creating a function that will traverse and mark the part of speech of each sentence as follows:\n"
      ],
      "metadata": {
        "id": "xVdhM6sJPcH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMj6X6_xPnXo",
        "outputId": "61326eef-7e35-49eb-b2de-a8f236b479de"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_content () :\n",
        "   try :\n",
        "      for i in tokenized[: 5 ]:\n",
        "         words = nltk.word_tokenize(i)\n",
        "         tagged = nltk.pos_tag(words)\n",
        "         print(tagged)\n",
        "   except Exception as e:\n",
        "        print(str(e))\n",
        "process_content()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-_VE5bMPebm",
        "outputId": "6f43037b-24c6-4596-ddc5-d686183c81d3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
            "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
            "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
            "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
            "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V. NLTK block\n",
        "Now that we know the part of speech, we can pay attention to the so-called block, which divides the word into meaningful blocks. One of the main goals of blocking is to group so-called “noun phrases”. These are phrases that contain one or more words of a noun, which may be descriptive words, a verb, or an adverb. The idea is to combine nouns and words related to them.\n",
        "\n",
        "In order to block, we combine the part-of-speech tag with a regular expression. Mainly from regular expressions, we want to take advantage of these things:\n",
        "\n"
      ],
      "metadata": {
        "id": "_xeVjNRkP22t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ = match 1 or more\n",
        "? = match 0 or 1 repetitions.\n",
        "* = match 0 or MORE repetitions   \n",
        ". = Any character except a new line"
      ],
      "metadata": {
        "id": "O6aOVqMtP5Af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "train_text = state_union.raw( \"2005-GWBush.txt\" )\n",
        "sample_text = state_union.raw( \"2006-GWBush.txt\" )\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
        "def process_content () :\n",
        "    try :\n",
        "        for i in tokenized:\n",
        "            words = nltk.word_tokenize(i)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
        "            chunkParser = nltk.RegexpParser(chunkGram)\n",
        "            chunked = chunkParser.parse(tagged)\n",
        "            chunked.draw()\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "process_content()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4f9SbdeP5br",
        "outputId": "6f90802a-c461-4211-c468-74aafcbb50e1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no display name and no $DISPLAY environment variable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main line here is:"
      ],
      "metadata": {
        "id": "YVGjzWQQQYgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\""
      ],
      "metadata": {
        "id": "FP_EzPw_QY6h"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split this line apart:\n",
        "\n",
        "<RB.?>* : Zero or more adverbs of any tense, followed by:\n",
        "\n",
        "<VB.?>* : Zero or more verbs of any tense, followed by:\n",
        "\n",
        "<NNP>+ : One or more reasonable nouns, followed by:\n",
        "\n",
        "<NN>? : Zero or one noun singular.\n",
        "\n",
        "Try a fun mix to group the various instances until you feel familiar.\n",
        "\n",
        "Not covered in the video, but there is also a reasonable task to actually access a specific block. This is rarely mentioned, but depending on what you are doing, this can be an important step. Suppose you print out the block and you will see the following output:"
      ],
      "metadata": {
        "id": "x9BKXI1aQe6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "( S ( Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP) 'S/POS ( Chunk ADDRESS/NNP BEFORE/NNP A/NNP JOINT/NNP SESSION/NNP OF/NNP THE/NNP CONGRESS/NNP ON/ NNP THE/NNP STATE/NNP OF/NNP THE/NNP UNION/NNP January/NNP) 31 /CD , /, 2006 /CD THE/DT ( Chunk PRESIDENT/NNP ) :/ : ( Chunk Thank/NNP) you/PRP All/DT ./.)"
      ],
      "metadata": {
        "id": "DwZe0DjaQh30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cool, this helps us visualize, but what if we want to access this data through our program? So what happens here is that our “blocking” variable is an NLTK tree. Each “block” and “non-block” is the “subtree” of the tree. We can refer to them by something like chunked.subtrees . Then we can iterate through these subtrees like this:"
      ],
      "metadata": {
        "id": "QLk6lR3jQkff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for subtree in chunked.subtrees():\n",
        "        print (subtree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "uHCzr6bPQhK0",
        "outputId": "51ea855c-265a-4373-fa1c-85f6fed37fce"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'chunked' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-d06ad31db6f7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msubtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtrees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msubtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'chunked' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "complete code"
      ],
      "metadata": {
        "id": "vo2g16tnQ7De"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "train_text = state_union.raw( \"2005-GWBush.txt\" )\n",
        "sample_text = state_union.raw( \"2006-GWBush.txt\" )\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
        "def process_content () :\n",
        "    try :\n",
        "        for i in tokenized:\n",
        "            words = nltk.word_tokenize(i)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
        "            chunkParser = nltk.RegexpParser(chunkGram)\n",
        "            chunked = chunkParser.parse(tagged)\n",
        "            print(chunked)\n",
        "            for subtree in chunked.subtrees(filter= lambda t: t.label() == 'Chunk' ):\n",
        "                print(subtree)\n",
        "            chunked.draw()\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "process_content()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AajWdXgFQtpS",
        "outputId": "868ca6f9-f3e1-4d9d-8861-14a4ce766f23"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
            "  'S/POS\n",
            "  (Chunk ADDRESS/NNP)\n",
            "  BEFORE/IN\n",
            "  (Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
            "  OF/IN\n",
            "  (Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
            "  OF/IN\n",
            "  (Chunk THE/NNP UNION/NNP January/NNP)\n",
            "  31/CD\n",
            "  ,/,\n",
            "  2006/CD\n",
            "  (Chunk THE/NNP PRESIDENT/NNP)\n",
            "  :/:\n",
            "  (Chunk Thank/NNP)\n",
            "  you/PRP\n",
            "  all/DT\n",
            "  ./.)\n",
            "(Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
            "(Chunk ADDRESS/NNP)\n",
            "(Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
            "(Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
            "(Chunk THE/NNP UNION/NNP January/NNP)\n",
            "(Chunk THE/NNP PRESIDENT/NNP)\n",
            "(Chunk Thank/NNP)\n",
            "no display name and no $DISPLAY environment variable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VI. NLTK adds a gap (Chinking)\n",
        "You may find that after a lot of chunking, there are some words in your block that you don’t want, but you don’t know how to get rid of them by blocking. You may find that adding a gap is your solution.\n",
        "\n",
        "Adding a gap is similar to a block, which is basically a way to remove a block from a block. The block you removed from the block is your gap.\n",
        "\n",
        "The code is very similar, you only need to use }{ to code the gap, behind the block, not the {} block.\n"
      ],
      "metadata": {
        "id": "0LfMXTbORBu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "train_text = state_union.raw( \"2005-GWBush.txt\" )\n",
        "sample_text = state_union.raw( \"2006-GWBush.txt\" )\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
        "def process_content () :\n",
        "    try :\n",
        "        for i in tokenized[ 5 :]:\n",
        "            words = nltk.word_tokenize(i)\n",
        "            Tagged = nltk.pos_tag(words)\n",
        "            chunkGram = r\"\"\"Chunk: {<.*>+} }<VB.?|IN|DT|TO>+{\"\"\"\n",
        "            chunkParser = nltk.RegexpParser(chunkGram)\n",
        "            chunked = chunkParser.parse(tagged)\n",
        "            chunked.draw()\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "process_content()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9sc3essRFH1",
        "outputId": "f7b6d931-bb95-4772-9e6d-79997d250560"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Illegal chunk pattern: {<.*>+} }<VB.?|IN|DT|TO>+{\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the main difference is:\n",
        "\n",
        "}<VB.?| IN |DT| TO >+{\n",
        "This means that we want to remove one or more verbs, prepositions, qualifiers or to words from the gap.\n",
        "\n",
        "Now that we have learned how to perform some custom partitioning and adding gaps, let’s discuss the block form that comes with NLTK, which is named entity recognition."
      ],
      "metadata": {
        "id": "eVOgQ69kRSxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VII. NLTK named entity recognition\n",
        "One of the most important forms of blocking in natural language processing is called “named entity recognition.” The idea is to let the machine immediately pull out “entities” such as people, places, things, locations, currencies, and more.\n",
        "\n",
        "This can be a challenge, but NLTK is built for us. NLTK’s named entity recognition has two main options: identifying all named entities, or identifying named entities as their respective types, such as person, location, location, etc.\n",
        "\n",
        "This is an example:"
      ],
      "metadata": {
        "id": "OPmi9TqfRXWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvObr25yRkgR",
        "outputId": "d3e3051f-8df3-4552-9303-12f6900c32b2"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsWTQtRmRoOb",
        "outputId": "14a312e7-1659-48cd-b816-c790305ee49f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "train_text = state_union.raw( \"2005-GWBush.txt\" )\n",
        "sample_text = state_union.raw( \"2006-GWBush.txt\" )\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
        "def process_content () :\n",
        "    try :\n",
        "        for i in tokenized[ 5 :]:\n",
        "            words = nltk.word_tokenize(i)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "            namedEnt = nltk.ne_chunk(tagged, binary= True )\n",
        "            namedEnt.draw()\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "process_content()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4imA10YRTcy",
        "outputId": "5e24b521-476b-46c2-d4e2-aa7b5e902325"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no display name and no $DISPLAY environment variable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see something right away. When binary is false, it also picks the same thing, but breaks the term White House into White and House as if they are different, and we can see it in the option binary = True , named entity The identification says that White House is part of the same named entity, which is correct.\n",
        "\n",
        "According to your goal, you can use the binary option. If your binary is false , here is what you can get, the type of named entity:"
      ],
      "metadata": {
        "id": "IetPP1bFRuC2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NE Type and Examples\n",
        "ORGANIZATION - Georgia -Pacific Corp . , WHO\n",
        "PERSON - Eddy Bonte, President Obama\n",
        "LOCATION - Murray River, Mount Everest\n",
        "DATE - June, 2008 - 06 - 29\n",
        "TIME - two fifty am, 1 : 30 p . m .\n",
        "MONEY - 175 million Canadian Dollars, GBP 10.40\n",
        "PERCENT - twenty pct, 18.75 %\n",
        "FACILITY - Washington Monument, Stonehenge\n",
        "GPE - South East Asia, Midlothian\n",
        "Either way, you may find that you need to do more work to get it right, but this feature is very powerful.\n",
        "\n",
        "In the next tutorial, we will discuss something similar to stem extraction, called “lemmatizing”."
      ],
      "metadata": {
        "id": "RPvDXxvtRxrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VIII. NLTK morphological restoration\n",
        "IX. NLTK Corpus\n",
        "X. NLTK and Wordnet\n",
        "XI. NLTK text classification\n",
        "XII. use NLTK to convert words into features\n",
        "XIII. NLTK Naive Bayes Classifier\n",
        "XIV. Save the classifier with NLTK\n",
        "XV. NLTK and Sklearn\n",
        "XVI. Using the NLTK combination algorithm\n",
        "XVII. Investigate bias using NLTK\n",
        "XVIII. Using NLTK to improve training data for sentiment analysis\n",
        "XIX. Use NLTK to create modules for sentiment analysis\n",
        "XX. NLTK Twitter sentiment analysis\n",
        "XXI. Using NLTK to draw Twitter real-time sentiment analysis\n",
        "XXII. Stanford NER marker and named entity recognition\n",
        "XXIII. Testing the accuracy of the NLTK and Stanford NER markers\n",
        "XXIV. testing the speed of the NLTK and Stanford NER markers\n",
        "https://medium.datadriveninvestor.com/python-data-science-getting-started-tutorial-nltk-2d8842fedfdd"
      ],
      "metadata": {
        "id": "rkNnjjl0SAWB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ojlbBG-cRtxs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}